{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "\n",
    "#### 1. 为什么线性回归中, 最小化的损失函数是最小二乘模型  \n",
    "线性回归, 假设函数为$y=\\theta x+\\varepsilon $, 其中$\\varepsilon $为估计值与真实值之间的误差.假设每个样本的估计误差满足高斯分布, 即$$P(\\varepsilon )=\\frac { 1 }{ \\sqrt { 2\\pi  } \\sigma  } exp\\left( -\\frac { { \\varepsilon  }^{ 2 } }{ 2{ \\sigma  }^{ 2 } }  \\right) $$上式表示$\\varepsilon$服从$N\\left( 0,{ \\sigma  }^{ 2 } \\right) $, 所以$y$服从$N\\left( \\theta x,{ \\sigma  }^{ 2 } \\right) $\n",
    "\n",
    "#### 2. 为什么假设$\\varepsilon$服从高斯分布\n",
    "假设估计误差满足高斯分布的原因有两个: 一个是中心极限定理指出, 许多独立随机变量值和服从高斯分布; 另一个是方便数学计算\n",
    "#### 3. 由最小二乘模型得出的损失函数  \n",
    "假设每个样本的估计误差满足独立同分布, 参数$\\theta $的极大似然估计为:$$L(\\theta )=P\\left( y|x;\\theta  \\right) =\\prod _{ i=1 }^{ m }{ \\frac { 1 }{ \\sqrt { 2\\pi  } \\sigma  } exp\\left( -\\frac { { { \\varepsilon  }_{ i } }^{ 2 } }{ 2{ \\sigma  }^{ 2 } }  \\right)  } \\\\ log\\left( L(\\theta ) \\right) =\\sum _{ i=1 }^{ m }{ \\log { \\frac { 1 }{ \\sqrt { 2\\pi  } \\sigma  }  } -\\frac { { { \\varepsilon  }_{ i } }^{ 2 } }{ 2{ \\sigma  }^{ 2 } }  } \\\\ max\\left\\{ log\\left( L(\\theta ) \\right)  \\right\\} =min\\left\\{ { { \\varepsilon  }_{ i } }^{ 2 } \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归\n",
    "\n",
    "#### 1. 逻辑回归参数的极大似然估计是什么   \n",
    "逻辑回归是2分类问题, 标签为0或1, 此时, 整个样本的标签分布满足伯努利分布$P(y=1)=\\phi $. 所以有: $$P\\left( { y }^{ 1 },{ y }^{ 2 }..{ y }^{ m } \\right) ={ \\phi  }^{ { y }^{ i } }{ \\left( 1-\\phi  \\right)  }^{ 1-{ y }^{ i } }$$经过对数似然, 得到最终的优化函数: $$max\\left( log\\left( L(\\theta ) \\right)  \\right) =max\\left\\{ { y }^{ i }log{ \\phi  }^{ i }+\\left( 1-{ y }^{ i } \\right) log\\left( 1-{ \\phi  }^{ i } \\right)  \\right\\} =min\\left\\{ { -y }^{ i }log{ \\phi  }^{ i }-\\left( 1-{ y }^{ i } \\right) log\\left( 1-{ \\phi  }^{ i } \\right)  \\right\\} $$\n",
    "\n",
    "#### 2. 逻辑回归的常用参数\n",
    "* penalty正则化项  \n",
    "    * 可选择L1或L2正则项  \n",
    "    * 由于L1正则项的梯度不是连续可导的, 因此参数solver只能选择\"liblinear\"\n",
    "* solver  \n",
    "    * 算法选择参数, 可选值 :  \n",
    "        * liblinear：使用了开源的liblinear库实现，使用坐标下降法迭代优化损失函数\n",
    "        * lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "        * newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "        * sag：即随机梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。\n",
    "    * 多分类下, 经常使用OvR或MvM2种方法. MvM分类精准但没有OvR快, 但MvM模式下, 不能使用liblinear优化损失函数\n",
    "* muti_class参数:多分类模式选择\n",
    "    * 默认值ovr\n",
    "        k种分类下训练k个分类器, 每个分类器都把自己的class作为正例, 其他class作为负例训练. 预测时看哪个分类器的得分高\n",
    "    * multinomial\n",
    "        k种分类下训练k(k-1)/2个分类器, 每个分类器只选择2个class的样本进行训练, 因此每个分类器只输出2个class中那个class的可能性大, 再综合所有分类器的结果输出\n",
    "* class_weight\n",
    " 当标签分布不均衡时, 通过调整权重指定正负例在梯度下降时的占比, 同时对预测准确率的计算变成加权计算  \n",
    " class_weight={0:0.9, 1:0.1}\n",
    "* sample_weight  \n",
    " 同样在样本分布不均匀时, 通过调整样本权重指定正负例在梯度下降时的占比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 逻辑回归与SVM的区别\n",
    "1. SVM使用hinge loss,而逻辑回归是交叉熵loss  \n",
    "2. SVM可以通过使用高斯核使分割超平面变成曲面  \n",
    "3. LR的分割面受所有点的影响, 而SVM只受到支持向量的影响"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
