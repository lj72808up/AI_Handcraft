{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "\n",
    "#### 1. 为什么线性回归中, 最小化的损失函数是最小二乘模型  \n",
    "线性回归, 假设函数为$y=\\theta x+\\varepsilon $, 其中$\\varepsilon $为估计值与真实值之间的误差.假设每个样本的估计误差满足高斯分布, 即$$P(\\varepsilon )=\\frac { 1 }{ \\sqrt { 2\\pi  } \\sigma  } exp\\left( -\\frac { { \\varepsilon  }^{ 2 } }{ 2{ \\sigma  }^{ 2 } }  \\right) $$上式表示$\\varepsilon$服从$N\\left( 0,{ \\sigma  }^{ 2 } \\right) $, 所以$y$服从$N\\left( \\theta x,{ \\sigma  }^{ 2 } \\right) $\n",
    "\n",
    "#### 2. 为什么假设$\\varepsilon$服从高斯分布\n",
    "假设估计误差满足高斯分布的原因有两个: 一个是中心极限定理指出, 许多独立随机变量值和服从高斯分布; 另一个是方便数学计算\n",
    "#### 3. 由最小二乘模型得出的损失函数  \n",
    "假设每个样本的估计误差满足独立同分布, 参数$\\theta $的极大似然估计为:$$L(\\theta )=P\\left( y|x;\\theta  \\right) =\\prod _{ i=1 }^{ m }{ \\frac { 1 }{ \\sqrt { 2\\pi  } \\sigma  } exp\\left( -\\frac { { { \\varepsilon  }_{ i } }^{ 2 } }{ 2{ \\sigma  }^{ 2 } }  \\right)  } \\\\ log\\left( L(\\theta ) \\right) =\\sum _{ i=1 }^{ m }{ \\log { \\frac { 1 }{ \\sqrt { 2\\pi  } \\sigma  }  } -\\frac { { { \\varepsilon  }_{ i } }^{ 2 } }{ 2{ \\sigma  }^{ 2 } }  } \\\\ max\\left\\{ log\\left( L(\\theta ) \\right)  \\right\\} =min\\left\\{ { { \\varepsilon  }_{ i } }^{ 2 } \\right\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归\n",
    "\n",
    "#### 1. 逻辑回归参数的极大似然估计是什么   \n",
    "逻辑回归是2分类问题, 标签为0或1, 此时, 整个样本的标签分布满足伯努利分布$P(y=1)=\\phi $. 所以有: $$P\\left( { y }^{ 1 },{ y }^{ 2 }..{ y }^{ m } \\right) ={ \\phi  }^{ { y }^{ i } }{ \\left( 1-\\phi  \\right)  }^{ { y }^{ i } }$$经过对数似然, 得到最终的优化函数: $$max\\left( log\\left( L(\\theta ) \\right)  \\right) =max\\left\\{ { y }^{ i }log{ \\phi  }^{ i }+\\left( 1-{ y }^{ i } \\right) log\\left( 1-{ \\phi  }^{ i } \\right)  \\right\\} =min\\left\\{ { -y }^{ i }log{ \\phi  }^{ i }-\\left( 1-{ y }^{ i } \\right) log\\left( 1-{ \\phi  }^{ i } \\right)  \\right\\} $$\n",
    "\n",
    "#### 2. 逻辑回归的常用参数\n",
    "* penalty  \n",
    " 选择L1或L2正则项  \n",
    "* solver  \n",
    " 算法选择参数, 可选值 :  \n",
    "    1. liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。\n",
    "    2. lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "    3. newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。\n",
    "    4. sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。\n",
    "* class_weight\n",
    " 当标签分布不均衡时, 通过调整权重指定正负例在梯度下降时的占比  \n",
    " class_weight={0:0.9, 1:0.1}\n",
    "* sample_weight  \n",
    " 同样在样本分布不均匀时, 通过调整样本权重指定正负例在梯度下降时的占比"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
