{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 一. 拉格朗日乘子\n",
    "\n",
    "#### 1. 最简单形式\n",
    "考虑这样一个问题, 要找到最大化函数$f\\left( { x }_{ 1 },{ x }_{ 2 } \\right) $时$x$的解, 并满足约束条件$g\\left( { x }_{ 1 },{ x }_{ 2 } \\right) =0$。  \n",
    "* 一种方法是 :   \n",
    " 将${ x }_{ 2 }$表示为${ x }_{ 1 }$的函数, 即${ x }_{ 2 }=h\\left( { x }_{ 1 } \\right) $, 带入方程求$f\\left( { x }_{ 1 },h\\left( { x }_{ 1 } \\right)  \\right) $的最大解. 这种方法的一个难点在于${ x }_{ 2 }$有时很难找到一个用${ x }_{ 1 }$显式表示的函数. 而且会破坏${ x }_{ 1 }$和${ x }_{ 2 }$的天然对称性  \n",
    "* 更优雅的方法 :   \n",
    " 将$g\\left( x \\right) =0$看做限制超平面, 其上面的点的梯度$\\nabla g\\left( x \\right) $与超平面正交. 且如果该限制平面上的点${ x }^{ * }$使得$f(x)$取得最大值, 则必满足$\\nabla g\\left( x \\right) $与$\\nabla f\\left( x \\right) $平行或逆向平行.(否则,沿限制超平面x移动一点距离就会使得$f(x)$增大一点). 有平行关系得出 : $$\\nabla f+\\lambda \\nabla g=0$$\n",
    " <img src=\"img/lagelangri1.PNG\" height=\"77%\" width=\"77%\">\n",
    " 由上述平行关系公式, 引入拉格朗日函数 : $$L\\left( x,\\lambda  \\right) \\equiv f\\left( x \\right) +\\lambda g\\left( x \\right) $$\n",
    " 该函数在$\\frac { \\partial L }{ \\partial x } =0$得到$\\nabla f+\\lambda \\nabla g=0$, 且在$\\frac { \\partial L }{ \\partial \\lambda  } =0$时得到限制超平面$g\\left( x \\right) =0$\n",
    " \n",
    "#### 2. 不等式条件下求解最大化函数\n",
    "到目前为止, 最大化函数$f(x)$是在等式约束条件$g(x)=0$下得出的, 如果约束条件为$g(x)\\succeq 0$时如何求解呢  \n",
    "此时求解分为2种情形 : \n",
    "* $g\\left( x \\right) >0$  \n",
    "这种情况下, 限制超平面$g(x)=0$不起作用, 解$x^*$只需满足$\\nabla f=0$. 仍使用拉格朗日函数表示$L\\left( x,\\lambda  \\right) \\equiv f\\left( x \\right) +\\lambda g\\left( x \\right) $, 但要求$\\lambda=0$\n",
    "* $g\\left( x \\right) =0$  \n",
    "这种情况下, 类比最简单情形的拉格朗日函数$L\\left( x,\\lambda  \\right) \\equiv f\\left( x \\right) +\\lambda g\\left( x \\right) $, 且$\\lambda \\neq 0$. 但要注意$\\lambda $的符号, 因为只有在梯度$\\nabla f$远离区域$g(x)>0$时, 函数$f(x)$才能取得最大值, 即$\\nabla f=-\\lambda \\nabla g$,且$\\lambda>0$  \n",
    "综合以上两种情形, 得出$\\lambda g\\left( x \\right) =0$. 将这个等式作为扩展条件, 得出求解最大化$f(x)$在$g\\left( x \\right) \\ge 0$约束下的3个条件$$\\begin{cases} g\\left( x \\right) \\ge 0 \\\\ \\lambda \\ge 0 \\\\ \\lambda g\\left( x \\right) =0 \\end{cases}$$称作KKT条件  \n",
    "需要注意, 若要最小化$f(x)$在$g\\left( x \\right) \\ge 0$约束下, 则拉格朗日函数变为$L\\left( x,\\lambda  \\right) \\equiv f\\left( x \\right) -\\lambda g\\left( x \\right) $,且$\\lambda \\ge 0$\n",
    "<img src=\"img/lagelangri2.PNG\" height=\"77%\" width=\"77%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二. 最大间隔分类器\n",
    "#### 1. margin  \n",
    "先从最简单的情形开始, 使用线性超平面进行二分类, 超平面为$$y\\left( x \\right) ={ W }^{ T }\\phi \\left( x \\right) +b$$其中$\\phi \\left( x \\right) $代表特征组合转换后的特征空间, 且显式的加上了偏差项$b$. 训练样本的标签集为$\\{-1,1\\}$; 预测新样本的标签需要判断$y(x)$的符号.  \n",
    "SVM规定, 决策边界到任何样本的最小距离为margin, 而SVM需要找到使margin最大时的参数求解\n",
    "#### 2. 距离  \n",
    "符号$t_n$表示样本的标签, $y(x_n)$表示预测值. 假定所有样本均被正确分类, 则对于所有样本有${ t }_{ n }y\\left( { x }_{ n } \\right) >0$  \n",
    "规定样本点到超平面的距离为$$\\frac { { t }_{ n }y\\left( { x }_{ n } \\right)  }{ \\parallel w\\parallel  } =\\frac { { t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right)  }{ \\parallel w\\parallel  } , \\quad \\quad 因为{ t }_{ n }只能是1或-1, 所以分子在保证永远为正的情况下绝对值不变$$\n",
    "因此, 最大化margin的问题变成解决如下函数$$\\underset { w,b }{ argmax } \\left\\{ \\frac { 1 }{ \\parallel w\\parallel  } \\underset { n }{ min } \\left[ { t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right)  \\right]  \\right\\} $$\n",
    "#### 3. 最优化函数的简化   \n",
    "直接求解上述最优化问题将很复杂. 注意到, 如果缩放$w\\rightarrow kw,b\\rightarrow kb$, 但点$x_n$到决策平面的距离$\\frac { { t }_{ n }y\\left( { x }_{ n } \\right)  }{ \\parallel w\\parallel  } $并不会变化, 因为分母归一化了. 利用这个特点, 我们规定, 离决策平面最近的点满足$${ t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right) =1$$即处于margin上的点, 等号成立, 距离为1, 其它所有点都满足${ t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right) \\ge 1, n=1..N$. 所以上述最大化margin的问题, 改为最小化${ \\parallel w\\parallel  }^{ 2 }$. 即最优化问题由上述的长式子变为短式子$$\\underset { w,b }{ argmax } \\left\\{ \\frac { 1 }{ \\parallel w\\parallel  } \\underset { n }{ min } \\left[ { t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right)  \\right]  \\right\\} \\quad \\longrightarrow \\quad \\begin{cases} \\underset { w,b }{ argmin } \\left\\{ \\frac { 1 }{ 2 } { \\parallel w\\parallel  }^{ 2 } \\right\\}  \\\\约束条件: { t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right) \\ge 1 \\end{cases}$$为求解这个最优化问题, 构造最小化拉格朗日函数$$L\\left( w,b,a \\right) =\\frac { 1 }{ 2 } { \\parallel w\\parallel  }^{ 2 }-\\sum _{ 1 }^{ n }{ { a }_{ n }\\left\\{ { t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right) -1 \\right\\}  } ,\\quad a_n是n个拉格朗日乘子$$分别对$w$和$b$求偏导=0, 得出$$w=\\sum _{ 1 }^{ n }{ { a }_{ n }{ t }_{ n }\\phi \\left( { x }_{ n } \\right)  } \\\\ 0=\\sum _{ 1 }^{ n }{ { a }_{ n }{ t }_{ n } } $$将解除的w和b带入拉格朗日函数, 得到最大化的新拉格朗日函数$$\\tilde { L } \\left( w,b,a \\right) =\\sum _{ 1 }^{ n }{ { a }_{ n } } -\\frac { 1 }{ 2 } \\sum _{ n=1 }^{ N }{ { a }_{ n }{ t }_{ n }\\phi \\left( { x }_{ n } \\right)  } \\sum _{ m=1 }^{ N }{ { a }_{ m }{ t }_{ m }\\phi \\left( { x }_{ m } \\right)  } \\\\ \\quad \\quad =\\sum _{ 1 }^{ n }{ { a }_{ n } } -\\frac { 1 }{ 2 } \\sum _{ n=1 }^{ N }{ \\sum _{ m=1 }^{ N }{ { a }_{ n } } { a }_{ m }{ t }_{ n }{ t }_{ m }k\\left( { x }_{ n },{ x }_{ m } \\right)  }$$\n",
    "把上述拉格朗日函数$\\tilde { L } \\left( w,b,a \\right) $中的$k\\left( { x },{ x }^{ ' } \\right) ={ \\phi \\left( { x } \\right)  }^{ T }\\phi \\left( { { x }^{ ' } } \\right) $称为核函数\n",
    "#### 4. 为何SVM适应稀疏矩阵求解  \n",
    "1. 注意到, SVM吧最优化问题, 从最初的最小化$\\underset { w,b }{ argmin } \\left\\{ \\frac { 1 }{ 2 } { \\parallel w\\parallel  }^{ 2 } \\right\\} $, 转变为最大化$\\tilde { L } \\left( w,b,a \\right) $. 如果特征维度为M,样本个数为N, 则前者的求解变量个数为M, 后者的求解变量个数为N. 即拉格朗日求解二次型问题后, 未知数求解个数由$M\\rightarrow N$. 如果特征维度远远大于样本个数则拉格朗日二次型求解转换带来求解复杂度降低, 否则这个转换反而加大求解复杂度  \n",
    "2. 注意要保证核函数$k\\left( { x },{ x }^{ ' } \\right) $为正数, 这样才是正确的拉格朗日最优化求解问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 为何SVM只对支持向量敏感\n",
    "因为上述求解偏导时得到$w=\\sum _{ 1 }^{ n }{ { a }_{ n }{ t }_{ n }\\phi \\left( { x }_{ n } \\right)  } $, 计算预测值时, 将样本数据带入$y\\left( { x } \\right) =w\\phi \\left( { x } \\right) +b$得$$y\\left( { x }_{ n } \\right) =\\sum _{ 1 }^{ n }{ { a }_{ n }{ t }_{ n }\\phi \\left( { x }_{ n } \\right)  } \\phi \\left( { x } \\right) +b\\\\ \\quad =\\sum _{ 1 }^{ n }{ { a }_{ n }{ t }_{ n }k\\left( { x }_{ n },x \\right)  } +b$$\n",
    "回想拉格朗日乘子, SVM最终构造的拉格朗日函数需要满足KKT条件\n",
    "<img src=\"img/kkt.PNG\" height=\"50%\" width=\"50%\">\n",
    "所以对于所有的数据点, 要么${ a }_{ n }=0$, 要么${ t }_{ n }y\\left( { x }_{ n } \\right) =1$. 而$a_n=0$的点在计算$y\\left( { x }_{ n } \\right) =\\sum _{ 1 }^{ n }{ { a }_{ n }{ t }_{ n }k\\left( { x }_{ n },x \\right)  } +b$时没有作用, 所以这些点对超平面没有作用. 而满足${ t }_{ n }y\\left( { x }_{ n } \\right) =1$的点称作支持向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 交叉分布的分类问题\n",
    "1. 到目前为止, 我们都在假设训练集在特征空间$\\phi \\left( { x } \\right) $下线性可分, 分类超平面给出确定的分割. 但实际中, 往往存在一些正负例交叉分布的形式, 这时需要修改SVM, 以适应存在分类错误的点\n",
    "2. 为此, 对样本点$x_n$引入惩罚函数${ \\varepsilon  }_{ n }$. 并规定:  \n",
    "  1. 如果样本点被正确分类,且在margin以外或在margin上, 则${ \\varepsilon  }_{ n }=0$  \n",
    "  2. 其它点(在margin以内或甚至分类错误的点), 有${ \\varepsilon  }_{ n }=\\left| { t }_{ n }-y\\left( { x }_{ n } \\right)  \\right| $, 即:\n",
    "    1. 如果样本点的$y\\left( { x }_{ n } \\right) =0$, 则${ \\varepsilon  }_{ n }=1$  \n",
    "    2. 如果样本点呗分类错误, 则${ \\varepsilon  }_{ n }>1$, 且数值随其距离正确分类的margin边缘的远近而线性增加\n",
    "3. 将${ t }_{ n }y\\left( { x }_{ n } \\right) \\ge 1$进行松弛, 得到条件${ t }_{ n }y\\left( { x }_{ n } \\right) \\ge 1-{ \\varepsilon  }_{ n }$. 且最优化问题从最小化$\\frac { 1 }{ 2 } { \\parallel w\\parallel  }^{ 2 }$变为$$最小化C\\sum _{ n=1 }^{ N }{ { \\varepsilon  }_{ n } } +\\frac { 1 }{ 2 } { \\parallel w\\parallel  }^{ 2 }$$参数C控制惩罚函数与最大间隔之间的平衡. 简化后的拉格朗日函数由$L\\left( w,b,a \\right) =\\frac { 1 }{ 2 } { \\parallel w\\parallel  }^{ 2 }-\\sum _{ 1 }^{ n }{ { a }_{ n }\\left\\{ { t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right) -1 \\right\\}  }$变为$$L\\left( w,b,a \\right) =\\frac { 1 }{ 2 } { \\parallel w\\parallel  }^{ 2 }+C\\sum _{ n=1 }^{ N }{ { \\varepsilon  }_{ n } } -\\sum _{ 1 }^{ n }{ { a }_{ n }\\left\\{ { t }_{ n }\\left( { W }^{ T }\\phi \\left( x \\right) +b \\right) +{ \\varepsilon  }_{ n }-1 \\right\\}  } -\\sum _{ n=1 }^{ N }{ { \\mu  }_{ n }{ \\varepsilon  }_{ n } }$$KKT条件:\n",
    "<img src=\"img/kkt2.PNG\" width=\"58%\" height=\"58%\">\n",
    "求偏到后:\n",
    "<img src=\"img/partial1.PNG\" width=\"52%\" height=\"52%\">  \n",
    "带入得到最终最大化拉格朗日函数\n",
    "<img src=\"img/touda1.PNG\" width=\"52%\" height=\"52%\">\n",
    "\n",
    "最终利用SMO求解最小化序列问题(sequential minimal optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 从hinge loss的角度看SVM损失函数  \n",
    "hinge loss为折线损失函数, 表达式为$$L(x)=max(0,1−x)$$ 其函数图形为<img src=\"img/hingeloss.PNG\" height=\"52%\" width=\"52%\">\n",
    "松弛变量后, SVM的最优化函数为$$\\underset { w,b }{ argmin } \\left\\{ C\\sum _{ n=1 }^{ N }{ { \\varepsilon  }_{ n } } +\\frac { 1 }{ 2 } { \\parallel w\\parallel  }^{ 2 } \\right\\} $$前面的${ \\varepsilon  }_{ n }$恰好满足  \n",
    " 1. 大于正确margin边界时为0  \n",
    " 2. 小于正确margin边界和分类错误时为$|t_n-y(x_n)|$  \n",
    "所以SVM的最优化函数为hinge loss和二次正则项的和"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
