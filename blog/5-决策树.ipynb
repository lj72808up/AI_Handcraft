{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "------------\n",
    "### 何为决策树  \n",
    "1. 决策树是基于树结构进行决策的，这恰是人类面临决策问题时的一种自然处理机制。决策过程的每个判定问题是对某个属性的\"测试\"。 \n",
    "2. 决策树由节点和有向边组成. 节点有两种类型: 内部节点和叶节点. 内部节点表示一个特征和属性,叶节点表示一个分类  \n",
    "3. 决策树本质上是从训练集中归纳出一个基于属性的分类规则. 与训练集不矛盾的决策树可能有多个,也可能一个没有. 我们需要找到一个与训练集矛盾最小的决策树, 同时具有很好的泛化能力.  \n",
    "4. 从损失函数的角度看问题: 决策树的损失函数通常是正则化的极大似然函数. 决策树的策略是以损失函数为目标函数的最小化问题. 而这个问题是一个NP完全问题 (指解决一个问题, 没有一个公式可以直接求出, 只能通过间接猜想来求解, 然后告诉你这个猜想结果是正确还是错误的). 现实中的决策树学习算法通常采用启发式近似求解, 这样得到的决策树是次最优的. 常用的ID3,C4.5,CART就是解决了决策树的特征选择,决策树的生成和剪枝过程.\n",
    "\n",
    "### 特征选择\n",
    "如果使用一个特征分割的结果,与随机分类产生的结果没有很大差距, 则称这个特征没有分类能力.而特征选择,就是要选取分类能力强的特征.分类能力可用消息增益或消息增益比来表示.\n",
    "\n",
    "### 信息增益\n",
    "1. 墒: 在信息论和概率统计中, 墒表示离散型随机变量不确定性的程度.即若$P(X={ x }_{ i })={ p }_{ i }$, 则$X$的墒$ H(X)=\\sum _{ i=1 }^{ k }{ { -p }_{ i }\\log { { p }_{ i } }  } $. 由此可知, 墒只与$X$的分布有关, 而与取值无关.熵越大, 随机变量的不确定性就越大\n",
    "\n",
    "### 信息增益比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART树\n",
    "---------\n",
    "### 一. CART用于回归\n",
    "1. 回归树的思路为: 我们通过训练, 把输入空间$X$划分为k个单元${ R }_{ 1 },{ R }_{ 2 }...{ R }_{ k }$. 并且在这个k个单元上, 固定一个输出值${ c }_{ k }$. 以后预测样本时, 只要确定了样本属于哪一个单元, 直接输出该单元对应的${ c }_{ k }$即可  \n",
    "2. 我们用平方误差表示训练集上的预测误差. 用平方误差最小化来确定该单元的最优输出值.因此, 单元${ R }_{ k }$上的最优解  $\\widehat { { c }_{ k } } $为所有属于该单元的样本${x}_{i}$的输出的均值. 即$\\widehat { { c }_{ k } } =ave({ y }_{ i }|{ x }_{ i }\\in { R }_{ k })$  \n",
    "3. 现在问题只剩如何划分$k$个单元. \n",
    "假设我们选取${x}^{(j)}$作为切分变量,${x}^{(j)}$的值作为切分点$s$,从而把训练集分成两个部分,$\\begin{cases} { R }_{ 1 }(j,s)=\\left\\{ x|{ x }^{ (j) }\\le s \\right\\}  \\\\ { R }_{ 2 }(j,s)=\\left\\{ x|{ x }^{ (j) }>s \\right\\}  \\end{cases}$.然后分别计算这两个单元上的输出均值$\\begin{cases} { { c }_{ 1 }= }ave\\left\\{ { y }^{ i }|{ { x }^{ i } }\\in { R }_{ 1 } \\right\\}  \\\\ { { c }_{ 2 }= }ave\\left\\{ { y }^{ i }|{ x }^{ i }\\in { R }_{ 2 } \\right\\}  \\end{cases}$. 而我们要找的最佳切分点${ x }^{ (j) }$, 就是最小化平方误差$min\\left[ \\sum _{ { x }^{ j }\\in { R }_{ 1 } }^{  }{ { ({ y }^{ j }-{ c }_{ 1 }) }^{ 2 } } +\\sum _{ { x }^{ j }\\in { R }_{ 2 } }^{  }{ { ({ y }^{ j }-{ c }_{ 2 }) }^{ 2 } }  \\right] $的样本点. 这种回归树也叫做最小二乘树    \n",
    "4. 利用这个方法, 我们不断使用二分分隔, 把样本点在所有属性上的取值, 依次二分单元, 然后递归的在上个单元再次进行划分.这种策略极易产生过拟合, 只要树的叶子节点只有一个样本时停止, 就能达到100%的回归准确, 单着并不意味着在测试集上拟合准确\n",
    "\n",
    "### 二. CART用于分类\n",
    "1. CART使用基尼系数选择当前最佳分类特征  \n",
    "数据集D的纯度使用基尼值来度量: $Gini(D)=1-\\sum _{ i=1 }^{ k }{ { { p }_{ i } }^{ 2 } } $. $Gini(D)$反映了从数据集D中随机抽取两个样本, 其类别标记不一致的概率, 因此$Gini(D)$数值越小, 样本纯度越高. 于是我们在候选属性集合$A$中, 选取那个划分后基尼数值最小的属性  \n",
    "2. 连续只处理  \n",
    "3. 缺失值处理  \n",
    "2. 分类树的伪代码  \n",
    "```python\n",
    "# 属性(feature)集A, 训练集D \n",
    "def TreeGenerate: \n",
    "    if 'D中样本全部属于同一类别c':\n",
    "        '将node标记为c类叶节点';return;\n",
    "    if 'A=空' or 'D中样本在A上取值相同':\n",
    "        '将node标记为D中类别最多的类';return;\n",
    "    '选择最优化分属性a'\n",
    "    for a in '属性a的每一个离散取值':\n",
    "        '生成一个分支,表示该分支上的样本子集Dv在属性a上去同一个值v'\n",
    "        '以TreeGenerate{Dv,A\\{a}}为分支节点递归构建树'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost对GBDT的优化\n",
    "1. GBDT是一种梯度提升方法, GBDT基于负梯度学习, xgboost基于二阶泰勒展开, 收敛速度更开, 训练速度更快\n",
    "2. 在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。\n",
    "3. xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。\n",
    "4. 特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。\n",
    "5. 按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。\n",
    "6. xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
