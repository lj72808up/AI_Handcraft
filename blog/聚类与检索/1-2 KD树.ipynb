{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 KD-Tree构建\n",
    "#### 一. KD-Tree的构建方法\n",
    "1. KD树是一种, 通过不同特征上的特征值, 把样本点构建成一个二叉树的方法. 二叉树的每个节点都是样本点的集合, 叶子节点是最终较为相似的样本点的集合, 他们拥有相似的特征值.\"KD\"表示样本点有k个特征, \"tree\"表示最终构建的是一个二叉树\n",
    "2. 如何构建kd-tree, 考虑以下三个问题:  \n",
    "    * which dimension do we split along?    \n",
    "        * widest or alternative dimension   \n",
    "          (选择取值范围最大的特征作为分叉的考量特征, 或交替使用不同特征进行分叉)    \n",
    "    * which value do we split at? \n",
    "        * median or center point of box ignoring data in box  \n",
    "          (选择中位数,或忽略数据分布选择$\\frac{range}{2}$)\n",
    "    * when do we stop?\n",
    "        * fewer than m points left or hits minimun width  \n",
    "          (叶子节点少于m个样本点,或属性达到了最小range)  \n",
    "    \n",
    "#### 二. KD-tree用于查找nearest neighbor\n",
    "1. 一旦构建好kd-tree后, 如果要对新的样本点进行nearest neighboor查找, 只要顺着kd-tree的节点划分条件, 逐层向下查找, 一直找到叶子节点, 该叶子节点为新样本所属的叶子节点\n",
    "2. 首先从新样本所属叶子节点的其他样本开始查找最邻近, 记录下最邻近节点距离n\n",
    "3. 然后回溯其他节点集合\n",
    " <img src=\"../../img/kdtree.png\" width=\"65%\" height=\"65%\">\n",
    "2. 对应文章检索, 上图说明, kd树的每个叶子节点都是一篇文章的向量表示.其中:  \n",
    "  1. 绿色点: 带查询文章\n",
    "  2. 蓝色框: 带查询文章所属的叶子节点包括的所有文章\n",
    "3. kd树搜索1-最临近的方法: \n",
    "  1. 找到带检索文章$x_q$所在的叶子节点, 比较该叶子节点上的文章与带检索文章的距离,取最近的一个作为1-nn\n",
    "  2. 回溯分支, 判断该分支的矩形边界中是否会有与待查文章$$x_q$$距离小于已知最临近距离$r$的文章. 方法是: \n",
    "    1. 从几何空间上看, 判断以$x_q$为球心, $r$为半径的球是否与该空间矩形边界有重叠(overlap)\n",
    "    2. 实际上, 在构造树的过程中, 记录下叶子节点的属性取值范围$[min,max]$,确保在所有属性的$x^{min}和x^{max}$上,与$x_q$的距离都小于$|{ x }_{ q }^{ k }-{ x }_{ NN }^{ k }|$, 否则对该分支剪枝, 不去计算该叶子节点内的文章距离\n",
    "  <img src=\"../../img/prunekd.png\"  width=\"60%\" height=\"60%\">\n",
    "  \n",
    "#### 三. KD树的搜索复杂度\n",
    "1. KD树构建的时间复杂度(N个叶子节点) : $Nlog(N)$   \n",
    "2. 1-NN搜索复杂度 : $O(log(N))$到$O(N)$\n",
    "3. KD树在多维空间下的效果并不好\n",
    "\n",
    "#### 四. kd树的近似k-nn搜索\n",
    "1. 设待查询节点$x_q$与所属叶子节点上最临近文档的距离为$r$. 在上述策略在回溯分支时, 会忽略存在文档距离$>r$的叶子节点.而KD-树近似搜索算法中, 要裁剪掉距离大于$\\frac { r }{ \\alpha  } (\\alpha >1)$的叶子节点. 这样做的意义在于:  \n",
    "  1. 当我们返回最近距离为$r$的最临近文档时, 保证没有距离大于$\\frac { r }{ \\alpha  }$的文档\n",
    "  2. 由于裁剪分支的规则更严格$(\\alpha >1)$, 所以会大大减少需要计算距离的文档数量(忽略整个叶子节点上的文档)\n",
    "\n",
    "2. 为什么KD-树不适用于高维数据\n",
    "  1. 当feature维度很高(d很大)时,由于每个feature都会至少包含两个分支, 所以最终构建一颗完整KD树所需要的样本点会很多($N>>2^d$). 二实际上, 往往没有这么多的样本. 而且高纬度下, 决策边界称为超立方体, 这些立方体往往都会有至少在一个属性维度上和已知最临近画出的超球体交叉, 使得剪枝效果大大下降\n",
    "  2. KD-树的距离计算对无关feature很敏感. 而很多feature往往作为噪声出现, 需要另外学习哪些feature较为重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
