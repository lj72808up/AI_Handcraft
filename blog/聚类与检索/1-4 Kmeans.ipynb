{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$4. K-Means聚类\n",
    "\n",
    "### 4.1 K-Means处理过程\n",
    "\n",
    "#### 一. k-means步骤\n",
    "1. 随机初始化K个分类中心, 用1,2 ... KRn表示\n",
    "2. 优化中心点\n",
    "<img src=\"../../img/kmeans1.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "#### 二. 优化目标\n",
    "1. 失真代价函数 (distortion cost function)  \n",
    " $J\\left( { c }^{ (1) },{ c }^{ (2) }...{ c }^{ (m) },{ \\mu  }_{ 1 },{ \\mu  }_{ 2 }...{ \\mu  }_{ k } \\right) =\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ { \\parallel { x }^{ (i) }-{ \\mu  }_{ c }^{ (i) }\\parallel  }^{ 2 } } $  \n",
    "2. 符号解释 :   \n",
    "  1. $c^{(i)}$ : 记录样本$x^{i}$当前被分配到的cluster编号   \n",
    "  2. ${ \\mu  }_{ k }$ : 记录第k个中心点的坐标  \n",
    "  3. ${ \\mu  }_{ { c }^{ (i) } }$ : 样本$x^{(i)}$被分配到的$cluster$的中心坐标  \n",
    "3. 鉴于k均值的这个损失函数格式, 和算法中每次cluster(簇)分配的策略一致, 所以在迭代过程中, k-means的cost function值一直是下降的\n",
    "\n",
    "#### 三. 初次cluster centroid坐标\n",
    "1. 随机选择K个样本, 并将1...K设置为这k个样本的坐标. 完成初始化\n",
    "2. 通常情况下, 初次cluster centroid的选择, 会导致最终的分类结果不同. 因此, 可以多初始化几次, 然后最终生成的多个cluster centroid, 计算失真函数的值, 选择一组失真函数最小的centroid作为最终结果\n",
    "\n",
    "#### 四. cluster数量选择\n",
    "1. 通常情况, 人们通过观察样本点的分散图或业务需求, 来人工决定分成几类  \n",
    " eg: 如果发现cluster数量K=5时, cost function的值比K=3的还要大, 这说明什么呢?  \n",
    " 通常情况, cluster数量越多, 总体的成本函数值越小. 但由于初始化centorid的关系, 也可能出现上述情况. 此时应该在K=5的设置中, 多初始化几次, 算法迭代后选择成本最小的cluster centroid\n",
    "2. 肘部法则  \n",
    " 这是一个相对科学的选择方法, 比如活出K值-cost function图像后, 找出折点(肘)最明显的点作为最终的分类数量. 单通的函数图像如下图右侧一样, 是光滑的, 此时这个方法也不管用\n",
    "3. [轮廓系数](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)\n",
    "    * 使用样本的平均轮廓系数衡量聚类效果, 综合考虑了离散度和凝聚度\n",
    "    * 样本轮廓系数范围在(-1,1).0以下表示聚类错误  \n",
    "    * 样本的轮廓系数计算方式为$$s=\\frac{b-a}{max(b,a)}$$\n",
    "        * a : 样本到同属一个cluster内的所有其他点的平均距离\n",
    "        * b : 样本到最邻近的其他cluster的所有点的平均距离  \n",
    "[编程作业](https://www.coursera.org/learn/ml-clustering-and-retrieval/supplement/E26k9/clustering-text-data-with-k-means)\n",
    "\n",
    " \n",
    "### 4.3 K-means的弊端\n",
    "1. 它是硬分配的(hard assignment).它把某篇文章确定的分到一个确定的类下, 而这个文章可能即是\"科技\"又是\"政治\",这种硬分配没有说出全部事实\n",
    "2. 它使用最小化样本点与聚类中心的欧氏距离来进行分配, 这已经在假设每个cluster都是以相同的圆形区域存在 (即: 每个cluster有相同轴比例的椭圆形)  \n",
    "<img src=\"../../img/k-meanslimitation.png\" width=\"65%\" height=\"65%\">\n",
    "3. k-means几个无法处理的场景  \n",
    "<img src=\"../../img/kcantresolve.png\" height=\"80%\" width=\"80%\">\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
