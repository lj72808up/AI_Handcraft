{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S1$. K-NN最临近-检索感兴趣的文章\n",
    "### 1.1 k-Nearest neighbor\n",
    "#### 一. k临近的输入与输出\n",
    "1. 输入:  \n",
    " 1. 待查询文档 $x_{q}$\n",
    " 2. 包含大量文章的语料库 $x_{1},x_{2}..x_{N}$  \n",
    "2. 输出:   \n",
    " 1. k个最相似文章的列表\n",
    "3. 待解决问题:  \n",
    " k个最临近的文档列表: ${ X }^{ NN }=\\left\\{ { X }^{ NN1 },{ X }^{ NN2 }...{ X }^{ NNm } \\right\\} $  \n",
    " for all $x_i$ not in ${ X }^{ NN }: distance(x_i,x_q) \\succeq max\\{distance(x^{NNj},x_q)\\}$  意为: 不在${ X }^{ NN }$集合中的文章, 其与$x_q$的距离要大于${ X }^{ NN }$中与$x_q$距离的最大值  \n",
    " \n",
    "#### 二. knn的伪代码\n",
    "```python\n",
    "knndists,kindexs = sort([d1,d2 .. dk]) # 计算语料库中前k个文章与待查询文章的距离,将其排序后,放入距离列表和文档index列表\n",
    "for i in range(k,N-1):  # 对剩下的第k+1篇文档到第N篇文档计算\n",
    "    d = distance(xi,x1)\n",
    "    if d<knndists[k-1]: # 如果新得到的距离小于k最临近的最后一个距离, 则并入knn列表 \n",
    "        find j such that d>knndists[j-1]&&d<knndists[j]:\n",
    "            # 删除距离最大的knndists的文章, 并把该文章放到j的位置\n",
    "            delete knndists[k-1] && move knndists[j,k-2] to knndists[j+1,k-1] && knndists[j]=d   \n",
    "            kindexs same action\n",
    "return knndists,kindexs\n",
    "```\n",
    "\n",
    "### 1.2 TF-IDF文档表示\n",
    "#### 一. 何为TF-IDF:  \n",
    "1. TF-IDF-词频逆文档频率 (Term Frequency inverse Document Frequency)\n",
    "2. TF-IDF把文章表示成一个向量.该向量的每个元素表示对应文章中该单词的出现次数(词频).如果单词表有10,000个单词, 则向量有10,000个元素,每个元素=词频*权重\n",
    "3. TF-IDF是突出重要单词的一种文档表示方法. 因为像\"the of\"这样的单词, 在文章中会大量出现.如果直接用其词频放在向量中, 则这样的单词会对最终计算k临近距离产生较大影响.因此向量中每个元素使用词频*权重表示.何为重要单词, 需要满足以下两个条件:   \n",
    "  1. 在本文章中大量出现(词频)  \n",
    "  2. 在语料库中的其他文章很少出现(词频) \n",
    "  \n",
    "#### 二. 强调重点单词 emphasize important words\n",
    "1. 局部地方频繁出现 - appear frequently in document (common locally)   \n",
    " TF : Term Frequency词频 = {x,x,x ... x}(10,000个单词,每个单词出现的次数)\n",
    "2. 语料库中很少出现 - rarely apperance in corpus (rarely global)  \n",
    " IDF : Inverse doc frequence逆文档频率 = $\\log { \\frac { \\#docs }{ 1+\\#doc\\quad using\\quad word }  } = \\log { \\frac { 语料库总文档数 }{ 1+使用该单词的文档数 }  }  $\n",
    "3. 文档表示向量 = $TF*IDF$ (词频*权重)\n",
    "\n",
    "### 1.3 距离度量\n",
    "有了文档表示向量, 再加上距离度量方法, 就能使用k临近算法了\n",
    "#### 一. 带权重的欧氏距离\n",
    "1. 带权距离: $distance({ x }_{ i },{ x }_{ q })=\\sqrt { { a }_{ 1 }{ \\left( { x }_{ i }[1]-{ x }_{ q }[1] \\right)  }^{ 2 }+{ a }_{ 2 }{ \\left( { x }_{ i }[2]-{ x }_{ q }[2] \\right)  }^{ 2 }+...+{ a }_{ 1 }{ \\left( { x }_{ i }[d]-{ x }_{ q }[d] \\right)  }^{ 2 } } $  \n",
    "  1. ${ a }_{ i }$: 表示每个词在计算距离时的权重.   \n",
    "   正如回归问题中的权重一样, 由于每个特征的取值范围不同,可能有某个特征的数值波动很大, 这回掩盖那些数值波动小的特征带来的影响, 然而那些小波动反而可能更有价值, 因此需要归一化. 比如特征值*$\\frac { 1 }{ { x }_{ max }-{ x }_{ min } } $,这个$\\frac { 1 }{ { x }_{ max }-{ x }_{ min } } $就是权值  \n",
    "  2. 一种取值预设方法:  \n",
    "   有一种预设权值的方法为: 二值化设定, 即权值只能为0或1. 这种方法相当于计算距离时的特征选择. 0就是不计入距离计算 (当然还有很多更合理的选择)\n",
    "   \n",
    "2. 矩阵计算   \n",
    "  1. $distance({ x }_{ i },{ x }_{ q })=\\left[ { x }_{ i }[1]-{ x }_{ q }[1],{ x }_{ i }[2]-{ x }_{ q }[2]...{ x }_{ i }[d]-{ x }_{ q }[d] \\right] \\begin{bmatrix} { a }_{ 1 } &  &  &  \\\\  & { a }_{ 2 } &  &  \\\\  &  & ... &  \\\\  &  &  & { a }_{ d } \\end{bmatrix}\\begin{bmatrix} { x }_{ i }[1]-{ x }_{ q }[1] \\\\ { x }_{ i }[2]-{ x }_{ q }[2] \\\\ ... \\\\ { x }_{ i }[d]-{ x }_{ q }[d] \\end{bmatrix}$  \n",
    "  2. 中间的对角矩阵为权值 \n",
    "  \n",
    "#### 二. Cosine similarity\n",
    "1. 向量相似度:  \n",
    "$similarity({ x }_{ i },{ x }_{ q }) = \\frac { { x }_{ i }*{ x }_{ q } }{ \\parallel { x }_{ i }\\parallel *\\parallel { x }_{ q }\\parallel  } $\n",
    "2. $\\frac { { x }_{ i }*{ x }_{ q } }{ \\parallel { x }_{ i }\\parallel *\\parallel { x }_{ q }\\parallel  } $相当于先用${ x }_{ i }*{ x }_{ q }$计算相似度, 再进行正则化. 而向量乘法之所以能进行计算相似度, 是因为若两个向量对应元素同号,则证明相近, 求和时带来增加效果, 若异号, 则带来减少效果. 那既然向量乘法已经能做法相似度比较, 那正则化还有作用吗?  \n",
    "  1. 不带正则化的向量乘会导致一个问题, 就是文章越长, 得到的乘法计算值越大(因为TF-IDF用词频表示,二文章越长,词频越大).所以使用正则化的cos计算相似度, 会忽略文章的长度  \n",
    "  2. cos计算相似度虽然会忽略文章长度, 但是在某些情况下, 他可能会把一篇很短的tweeter推文, 和一篇鸿篇巨著的paper处理成类似而推荐给阅读者. 但是喜欢简短推文的人往往不会喜欢看鸿篇巨制. 为了解决这个问题, 于是在正则化和非正则化之间做一个均衡, 限制词频的最大值,来避免文章长度差别过大\n",
    "  \n",
    "#### 三. 欧氏距离和Cosine similarity在文章检索的适用场景\n",
    "1. 若根据文章内容识别文章相似度, 则使用COsine计算. 可以避免文章长度的影响做出合适的相似度检索\n",
    "2. 若根据某人对文章的阅读次数来识别文章相似度, 则使用不去正则化的欧式距离较为合适\n",
    "\n",
    "### 1.4 KNN的搜索复杂度(search complexity)\n",
    "#### 一. KNN的时间复杂度\n",
    "1. 1-NN: $O(N)$ : $N$是语料库中文章个数\n",
    "2. k-NN: $O(Nlogk)$ : $N$是语料库中文章个数. $O(logk)$因为若用优先队列存储前k个结果, 则每次插入队列需要$O(logk)$  \n",
    " 当N很大时,k-NN的时间复杂度会很高.因此这种存储方式需要改进.由此引出KD-Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $\\S$2. KD-Tree\n",
    "### 2.1 KD-Tree构建\n",
    "#### 一. KD-Tree的构建方法\n",
    "1. KD树把文档所在的空间分区, 分区平面平行于坐标轴. 每个分区内的点(文档)存在一个列表中.\n",
    "2. KD树是一个二叉树, 以某个属性的某一取值作为划分分支的判断标准,从而构建完整的KD树. KD树的叶子节点是最终划分区域所包含的文档.构建KD树根据启发式算法构建, 其三个问题如下:  \n",
    "  1. 选择哪个feature作为划分属性  \n",
    "   选择取值范围最大的feature有限划分  \n",
    "  2. 划分属性的取值  \n",
    "    1. 取该feature的中位数数值\n",
    "    2. 或者是centoer值作为划分取值.(后者会忽略样本分布情况)  \n",
    "  3. 什么时候停止划分    \n",
    "    1. 带划分区域的样本点个数小于预设值m  \n",
    "    2. 或者带划分属性取值宽度达到阈值  \n",
    "    \n",
    "#### 二. KD树查找最临近点\n",
    "1. 一颗构建好的KD树是一个二叉树, 通过属性取值划分, 如下图  \n",
    " <img src=\"../../img/kdtree.png\" width=\"65%\" height=\"65%\">\n",
    "2. 对应文章检索, 上图说明, kd树的每个叶子节点都是一篇文章的向量表示.其中:  \n",
    "  1. 绿色点: 带查询文章\n",
    "  2. 蓝色框: 带查询文章所属的叶子节点包括的所有文章\n",
    "3. kd树搜索1-最临近的方法: \n",
    "  1. 找到带检索文章$x_q$所在的叶子节点, 比较该叶子节点上的文章与带检索文章的距离,取最近的一个作为1-nn\n",
    "  2. 回溯分支, 判断该分支的矩形边界中是否会有与待查文章$$x_q$$距离小于已知最临近距离$r$的文章. 方法是: \n",
    "    1. 从几何空间上看, 判断以$x_q$为球心, $r$为半径的球是否与该空间矩形边界有重叠(overlap)\n",
    "    2. 实际上, 在构造树的过程中, 记录下叶子节点的属性取值范围$[min,max]$,确保在所有属性的$x^{min}和x^{max}$上,与$x_q$的距离都小于$|{ x }_{ q }^{ k }-{ x }_{ NN }^{ k }|$, 否则对该分支剪枝, 不去计算该叶子节点内的文章距离\n",
    "  <img src=\"../../img/prunekd.png\"  width=\"60%\" height=\"60%\">\n",
    "  \n",
    "#### 三. KD树的搜索复杂度\n",
    "1. KD树构建的时间复杂度(N个叶子节点) : $Nlog(N)$   \n",
    "2. 1-NN搜索复杂度 : $O(log(N))$到$O(N)$\n",
    "3. KD树在多维空间下的效果并不好\n",
    "\n",
    "#### 四. kd树的近似k-nn搜索\n",
    "1. 设待查询节点$x_q$与所属叶子节点上最临近文档的距离为$r$. 在上述策略在回溯分支时, 会忽略存在文档距离$>r$的叶子节点.而KD-树近似搜索算法中, 要裁剪掉距离大于$\\frac { r }{ \\alpha  } (\\alpha >1)$的叶子节点. 这样做的意义在于:  \n",
    "  1. 当我们返回最近距离为$r$的最临近文档时, 保证没有距离大于$\\frac { r }{ \\alpha  }$的文档\n",
    "  2. 由于裁剪分支的规则更严格$(\\alpha >1)$, 所以会大大减少需要计算距离的文档数量(忽略整个叶子节点上的文档)\n",
    "\n",
    "2. 为什么KD-树不适用于高维数据\n",
    "  1. 当feature维度很高(d很大)时,由于每个feature都会至少包含两个分支, 所以最终构建一颗完整KD树所需要的样本点会很多($N>>2^d$). 二实际上, 往往没有这么多的样本. 而且高纬度下, 决策边界称为超立方体, 这些立方体往往都会有至少在一个属性维度上和已知最临近画出的超球体交叉, 使得剪枝效果大大下降\n",
    "  2. KD-树的距离计算对无关feature很敏感. 而很多feature往往作为噪声出现, 需要另外学习哪些feature较为重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$3. 局部敏感哈希-Localy Sensitive Hash\n",
    "### 3.1 单个Hash表\n",
    "#### 一. 何为局部敏感哈希\n",
    "1. 局部敏感哈希认为, 相似的两个向量之间的夹角很小. 假设样本点只有2个feature, 此时样本点为2d图形. 当我们随机的用L条直线分割这些点, 每2条直线就会形成一个区域(bin).而因为2个最临近向量之间的夹角很小, 所以他们很有可能落在同一个bin上.\n",
    "<img src=\"../../img/lsh1.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "\n",
    "#### 二. LSH的步骤\n",
    "1. 随机生成L条直线划分样本点.  \n",
    " 此时产生问题: 2个NN向量, 没被随机划分到一个bin的概率是多少?  \n",
    "  1. 假设2个最临近向量之间夹角为$\\theta$,当存在一条直线穿过2个向量之间的夹角时, 2个NN向量就不在同一个bin中.  \n",
    "  2. 设随机分割直线落在两个点之间概率为$\\vartheta $, 则$\\vartheta =\\frac { \\theta  }{ \\pi  } $   \n",
    "  3. 因此2个NN向量, 没被随机划分到一个bin的概率为$1-\\vartheta =1-\\frac { \\theta  }{ \\pi  } $   \n",
    "2. 生成哈希表  \n",
    " 我们根据这些生成的随机直线, 计算每个样本点落在哪两条直线形成的bin中, 形成一个\"key=bin序号,value=样本点index\"的哈希表\n",
    "  <img src=\"../../img/lsh2.png\" width=\"60%\" height=\"60%\">\n",
    "3. 考察3个bin中的点  \n",
    " 因为有一定概率的划分, 使得真正的最临近点不在一个bin中. 因此, 计算最临近时,不仅要考察同一个bin内的样本点距离, 还要考察前后一个bin中的样本点.  \n",
    "  <img src=\"../../img/lsh4.png\" width=\"60%\" height=\"60%\">\n",
    "  \n",
    "  \n",
    "### 3.2 多张哈希表提升表现\n",
    "#### 一. 3张哈希表2条分割线的表现\n",
    "1. 生成一张哈希表时, 若我们用2条直线划分样本点,则把2个最临近点划分到不同bin的概率是多少?  \n",
    " $P=1-P(同一个bin)=1-p(两条直线都没在\\theta 角区域上)=1-{ (1-\\vartheta ) }^{ 2 }=2\\vartheta -{ \\vartheta  }^{ 2 }$  \n",
    "2. 若我们用上述步骤生成3个哈希表, 则2个最临近点被3词划分到不同bin的概率是多少?  \n",
    " $P={(2\\vartheta -{ \\vartheta  }^{ 2 })}^3$  \n",
    " <img src=\"../../img/lsh5.png\" width=\"90%\" height=\"90%\">\n",
    "3. 经过对比发现3张哈希表的表现, 在$\\vartheta =0.16$,即$夹角\\theta<=0.16\\pi$时, 表现好过1张哈希表\n",
    " <img src=\"../../img/lsh6.png\" width=\"70%\" height=\"70%\">\n",
    " \n",
    " \n",
    "#### 二. m张哈希表h条直线的表现\n",
    "1. 最临近点在所有m张表中都和待查点$x_q$不在同一个bin下的概率:  \n",
    "$P=1-P(same-bin)=(1-p(no-split-line))={ (1-{ (1-\\vartheta ) }^{ h }) }^{ m }$\n",
    "\n",
    "#### 三. 总结\n",
    "local sensitive hash有两种策略: \n",
    "1. 多条随机分割线,少张表  \n",
    " 这种情况下, 分配样本点到每个bin的开销小, 但是在查询时需要计算很多相邻的bin\n",
    "2. 少条随机分割线,多张表  \n",
    " 这种情况, 分配样本点到每个bin的开销大, 但每张表可以只要查询1个bin  \n",
    " \n",
    "[编程任务](https://www.coursera.org/learn/ml-clustering-and-retrieval/supplement/mMipk/implementing-locality-sensitive-hashing-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $\\S$4. K-Means聚类\n",
    "\n",
    "### 4.1 K-Means处理过程\n",
    "\n",
    "#### 一. k-means步骤\n",
    "1. 随机初始化K个分类中心, 用1,2 ... KRn表示\n",
    "2. 优化中心点\n",
    "<img src=\"../../img/kmeans1.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "#### 二. 优化目标\n",
    "1. 失真代价函数 (distortion cost function)  \n",
    " $J\\left( { c }^{ (1) },{ c }^{ (2) }...{ c }^{ (m) },{ \\mu  }_{ 1 },{ \\mu  }_{ 2 }...{ \\mu  }_{ k } \\right) =\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ { \\parallel { x }^{ (i) }-{ \\mu  }_{ c }^{ (i) }\\parallel  }^{ 2 } } $  \n",
    "2. 符号解释 :   \n",
    "  1. $c^{(i)}$ : 记录样本$x^{i}$当前被分配到的cluster编号   \n",
    "  2. ${ \\mu  }_{ k }$ : 记录第k个中心点的坐标  \n",
    "  3. ${ \\mu  }_{ { c }^{ (i) } }$ : 样本$x^{(i)}$被分配到的$cluster$的中心坐标  \n",
    "3. 鉴于k均值的这个损失函数格式, 和算法中每次cluster(簇)分配的策略一致, 所以在迭代过程中, k-means的cost function值一直是下降的\n",
    "\n",
    "#### 三. 初次cluster centroid坐标\n",
    "1. 随机选择K个样本, 并将1...K设置为这k个样本的坐标. 完成初始化\n",
    "2. 通常情况下, 初次cluster centroid的选择, 会导致最终的分类结果不同. 因此, 可以多初始化几次, 然后最终生成的多个cluster centroid, 计算失真函数的值, 选择一组失真函数最小的centroid作为最终结果\n",
    "\n",
    "#### 四. cluster数量选择\n",
    "1. 通常情况, 人们通过观察样本点的分散图或业务需求, 来人工决定分成几类  \n",
    " eg: 如果发现cluster数量K=5时, cost function的值比K=3的还要大, 这说明什么呢?  \n",
    " 通常情况, cluster数量越多, 总体的成本函数值越小. 但由于初始化centorid的关系, 也可能出现上述情况. 此时应该在K=5的设置中, 多初始化几次, 算法迭代后选择成本最小的cluster centroid\n",
    "2. 肘部法则  \n",
    " 这是一个相对科学的选择方法, 比如活出K值-cost function图像后, 找出折点(肘)最明显的点作为最终的分类数量. 单通的函数图像如下图右侧一样, 是光滑的, 此时这个方法也不管用\n",
    "\n",
    "[编程作业](https://www.coursera.org/learn/ml-clustering-and-retrieval/supplement/E26k9/clustering-text-data-with-k-means)\n",
    "\n",
    "### 4.2 大规模k-means的mapreduce算法\n",
    "\n",
    "#### 一. MapReduce的Wordount\n",
    "1. Mapper : 生成键值对(word,1)  \n",
    " 10亿个文本发往到100台mapper机器, 对每篇文章的每个单词生成\"(word,1)\"的键值对.  \n",
    "2. Reducer :   \n",
    " 1. 通过计算hash(key)方法, 把键值对发往不同Reducer(这个hash操作就是shuffle阶段)\n",
    " 2. 在上面进行sum([1,1,1...1])操作  \n",
    "3. 为了避免过多的键值对发往reducer, 可以再mapper上尽心一次小规模的reduce. 在mapper端进行(word,sum([1,1...1]))的操作, 将最终结果(word,#word)发往reducer. 此操作称为Combiner\n",
    " <img src=\"../../img/mr1.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "#### 二. Mapduce的K-Means  \n",
    "1. Mapper  \n",
    " 计算每篇文章所述的centor,将同一个centor的文章向量发往同一个reducer  \n",
    "2. Reducer  \n",
    " 在Reducer上重新计算新的$centorId=\\frac { \\sum { { x }^{ (i) } }  }{ n } $\n",
    " \n",
    "### 4.3 K-means的弊端\n",
    "1. 它是硬分配的(hard assignment).它把某篇文章确定的分到一个确定的类下, 而这个文章可能即是\"科技\"又是\"政治\",这种硬分配没有说出全部事实\n",
    "2. 它使用最小化样本点与聚类中心的欧氏距离来进行分配, 这已经在假设每个cluster都是以相同的圆形区域存在 (即: 每个cluster有相同轴比例的椭圆形)  \n",
    "<img src=\"../../img/k-meanslimitation.png\" width=\"65%\" height=\"65%\">\n",
    "3. k-means几个无法处理的场景  \n",
    "<img src=\"../../img/kcantresolve.png\" height=\"80%\" width=\"80%\">\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## $\\S$5. EM算法\n",
    "\n",
    "### 5.1 高斯分布的背景\n",
    "\n",
    "#### 一. 多元高斯分布\n",
    "\n",
    "1. 多元高斯分布的概率密度函数 : ${ f }_{ X }({ x }_{ 1 }...{ x }_{ k })=\\frac { 1 }{ \\sqrt { { (2\\pi ) }^{ k }|\\Sigma | }  } exp\\left[ -\\frac { 1 }{ 2 } { (X-\\mu ) }^{ T }{ \\Sigma  }^{ -1 }(X-\\mu ) \\right] $  \n",
    "\n",
    "#### 二. 应用场景  \n",
    "1. 假设有一堆图片,主题分别是\"海滩\",\"天空\",\"建筑\".要把这些图片自动的分成3类(cluster后没有这些标签).一个思路是从图片的RGB值入手.  \n",
    "2. 比如, 我们对每个图片的B(蓝色)占比画出直方图(histogram),发现不同label的图片在蓝色数值上展示出了不同的一元高斯分布\n",
    " <img src=\"../../img/gussian1.png\" width=\"50%\" height=\"50%\">\n",
    "3. 而当我们组合这三种图片的一元高斯分布直方图后, 形成一个起伏的联合直方图如下, 就是所谓的高斯混合.\n",
    "  1. 3个不同随机变量组合为1个随机变量的本质, 是对他们进行凸组合.由于随机变量的概率之和为1,我们应该为这3个最初的随机变量分配权重  \n",
    "  2. 用${ \\Pi  }_{ k }$表示不同cluster的权重, ${ \\Pi  }=\\left[ { \\pi  }_{ 1 },{ \\pi  }_{ 2 },{ \\pi  }_{ 3 } \\right] $\n",
    "<img src=\"../../img/gaussian2.png\" width=\"90%\" height=\"90%\">\n",
    "\n",
    "3. 以上所有图, 都只有Blue一个属性的分布, 这样做是为了简化表示这些图片的分类方式. 真正使用的应该是RGB三种颜色多元高斯分布建模.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
