{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S1$. 最邻近搜索-检索感兴趣的文章\n",
    "### 1.1 k-Nearest neighbor\n",
    "#### 一. k临近的输入与输出\n",
    "1. 输入:  \n",
    " 1. 待查询文档 $x_{q}$\n",
    " 2. 包含大量文章的语料库 $x_{1},x_{2}..x_{N}$  \n",
    "2. 输出:   \n",
    " 1. k个最相似文章的列表\n",
    "3. 待解决问题:  \n",
    " k个最临近的文档列表: ${ X }^{ NN }=\\left\\{ { X }^{ NN1 },{ X }^{ NN2 }...{ X }^{ NNm } \\right\\} $  \n",
    " for all $x_i$ not in ${ X }^{ NN }: distance(x_i,x_q) \\succeq max\\{distance(x^{NNj},x_q)\\}$  意为: 不在${ X }^{ NN }$集合中的文章, 其与$x_q$的距离要大于${ X }^{ NN }$中与$x_q$距离的最大值  \n",
    " \n",
    "#### 二. knn的伪代码\n",
    "```python\n",
    "knndists,kindexs = sort([d1,d2 .. dk]) # 计算语料库中前k个文章与待查询文章的距离,将其排序后,放入距离列表和文档index列表\n",
    "for i in range(k,N-1):  # 对剩下的第k+1篇文档到第N篇文档计算\n",
    "    d = distance(xi,x1)\n",
    "    if d<knndists[k-1]: # 如果新得到的距离小于k最临近的最后一个距离, 则并入knn列表 \n",
    "        find j such that d>knndists[j-1]&&d<knndists[j]:\n",
    "            # 删除距离最大的knndists的文章, 并把该文章放到j的位置\n",
    "            delete knndists[k-1] && move knndists[j,k-2] to knndists[j+1,k-1] && knndists[j]=d   \n",
    "            kindexs same action\n",
    "return knndists,kindexs\n",
    "```\n",
    "\n",
    "### 1.2 TF-IDF文档表示\n",
    "#### 一. 何为TF-IDF:  \n",
    "1. TF-IDF-词频逆文档频率 (Term Frequency inverse Document Frequency)\n",
    "2. TF-IDF把文章表示成一个向量.该向量的每个元素表示对应文章中该单词的出现次数(词频).如果单词表有10,000个单词, 则向量有10,000个元素,每个元素=词频*权重\n",
    "3. TF-IDF是突出重要单词的一种文档表示方法. 因为像\"the of\"这样的单词, 在文章中会大量出现.如果直接用其词频放在向量中, 则这样的单词会对最终计算k临近距离产生较大影响.因此向量中每个元素使用词频*权重表示.何为重要单词, 需要满足以下两个条件:   \n",
    "  1. 在本文章中大量出现(词频)  \n",
    "  2. 在语料库中的其他文章很少出现(词频) \n",
    "  \n",
    "#### 二. 强调重点单词 emphasize important words\n",
    "1. 局部地方频繁出现 - appear frequently in document (common locally)   \n",
    " TF : Term Frequency词频 = {x,x,x ... x}(10,000个单词,每个单词出现的次数)\n",
    "2. 语料库中很少出现 - rarely apperance in corpus (rarely global)  \n",
    " IDF : Inverse doc frequence逆文档频率 = $\\log { \\frac { \\#docs }{ 1+\\#doc\\quad using\\quad word }  } = \\log { \\frac { 语料库总文档数 }{ 1+使用该单词的文档数 }  }  $\n",
    "3. 文档表示向量 = $TF*IDF$ (词频*权重)\n",
    "\n",
    "### 1.3 距离度量\n",
    "有了文档表示向量, 再加上距离度量方法, 就能使用k临近算法了\n",
    "#### 一. 带权重的欧氏距离\n",
    "1. 带权距离: $distance({ x }_{ i },{ x }_{ q })=\\sqrt { { a }_{ 1 }{ \\left( { x }_{ i }[1]-{ x }_{ q }[1] \\right)  }^{ 2 }+{ a }_{ 2 }{ \\left( { x }_{ i }[2]-{ x }_{ q }[2] \\right)  }^{ 2 }+...+{ a }_{ 1 }{ \\left( { x }_{ i }[d]-{ x }_{ q }[d] \\right)  }^{ 2 } } $  \n",
    "  1. ${ a }_{ i }$: 表示每个词在计算距离时的权重.   \n",
    "   正如回归问题中的权重一样, 由于每个特征的取值范围不同,可能有某个特征的数值波动很大, 这回掩盖那些数值波动小的特征带来的影响, 然而那些小波动反而可能更有价值, 因此需要归一化. 比如特征值*$\\frac { 1 }{ { x }_{ max }-{ x }_{ min } } $,这个$\\frac { 1 }{ { x }_{ max }-{ x }_{ min } } $就是权值  \n",
    "  2. 一种取值预设方法:  \n",
    "   有一种预设权值的方法为: 二值化设定, 即权值只能为0或1. 这种方法相当于计算距离时的特征选择. 0就是不计入距离计算 (当然还有很多更合理的选择)\n",
    "   \n",
    "2. 矩阵计算   \n",
    "  1. $distance({ x }_{ i },{ x }_{ q })=\\left[ { x }_{ i }[1]-{ x }_{ q }[1],{ x }_{ i }[2]-{ x }_{ q }[2]...{ x }_{ i }[d]-{ x }_{ q }[d] \\right] \\begin{bmatrix} { a }_{ 1 } &  &  &  \\\\  & { a }_{ 2 } &  &  \\\\  &  & ... &  \\\\  &  &  & { a }_{ d } \\end{bmatrix}\\begin{bmatrix} { x }_{ i }[1]-{ x }_{ q }[1] \\\\ { x }_{ i }[2]-{ x }_{ q }[2] \\\\ ... \\\\ { x }_{ i }[d]-{ x }_{ q }[d] \\end{bmatrix}$  \n",
    "  2. 中间的对角矩阵为权值 \n",
    "  \n",
    "#### 二. Cosine similarity\n",
    "1. 向量相似度:  \n",
    "$similarity({ x }_{ i },{ x }_{ q }) = \\frac { { x }_{ i }*{ x }_{ q } }{ \\parallel { x }_{ i }\\parallel *\\parallel { x }_{ q }\\parallel  } $\n",
    "2. $\\frac { { x }_{ i }*{ x }_{ q } }{ \\parallel { x }_{ i }\\parallel *\\parallel { x }_{ q }\\parallel  } $相当于先用${ x }_{ i }*{ x }_{ q }$计算相似度, 再进行归一化. 而向量乘法之所以能进行计算相似度, 是因为若两个向量对应元素同号,则证明相近, 求和时带来增加效果, 若异号, 则带来减少效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
